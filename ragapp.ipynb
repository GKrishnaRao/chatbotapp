{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF: 32\n"
     ]
    }
   ],
   "source": [
    "# Load and read PDF content\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "loader = PDFPlumberLoader(\"pdf/FSI-Redevelopment.pdf\")\n",
    "docs = loader.load()\n",
    "print(\"Number of pages in the PDF:\", len(docs))\n",
    "\n",
    "# Extract text from each page for embedding\n",
    "doc_texts = [page.page_content for page in docs]\n",
    "#print(\" Document  texts\",doc_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishna/Documents/PROJECTS/pythonproject/Github/chatbotapp/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 189787.51it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from milvus_model.hybrid import BGEM3EmbeddingFunction\n",
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "ef = BGEM3EmbeddingFunction(use_fp16=False, device=device)\n",
    "dense_dim = ef.dim[\"dense\"]\n",
    "\n",
    "# Generate embeddings using BGE-M3 model\n",
    "docs_embeddings = ef(doc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Connect to Milvus given URI\n",
    "connections.connect(uri=\"./milvus.db\")\n",
    "\n",
    "# Specify the data schema for the new Collection\n",
    "fields = [\n",
    "    # Use auto generated id as primary key\n",
    "    FieldSchema(\n",
    "        name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100\n",
    "    ),\n",
    "    # Store the original text to retrieve based on semantically distance\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=2048),\n",
    "    # Milvus now supports both sparse and dense vectors,\n",
    "    # we can store each in a separate field to conduct hybrid search on both vectors\n",
    "    FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "    FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=dense_dim),\n",
    "]\n",
    "schema = CollectionSchema(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection (drop the old one if exists)\n",
    "col_name = \"hybrid_demo\"\n",
    "if utility.has_collection(col_name):\n",
    "    Collection(col_name).drop()\n",
    "col = Collection(col_name, schema, consistency_level=\"Strong\")\n",
    "\n",
    "# To make vector search efficient, we need to create indices for the vector fields\n",
    "sparse_index = {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}\n",
    "col.create_index(\"sparse_vector\", sparse_index)\n",
    "dense_index = {\"index_type\": \"AUTOINDEX\", \"metric_type\": \"IP\"}\n",
    "col.create_index(\"dense_vector\", dense_index)\n",
    "col.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities inserted: 32\n"
     ]
    }
   ],
   "source": [
    "# For efficiency, we insert 50 records in each small batch\n",
    "for i in range(0, len(doc_texts), 50):\n",
    "    # Extract the raw text from Document objects\n",
    "    batched_texts = [doc.page_content for doc in docs[i : i + 50]]  # Extract the text content\n",
    "    batched_sparse_vectors = docs_embeddings[\"sparse\"][i : i + 50]\n",
    "    batched_dense_vectors = docs_embeddings[\"dense\"][i : i + 50]\n",
    "\n",
    "    # Prepare the batched entities for insertion\n",
    "    batched_entities = [\n",
    "        batched_texts,          # Text content\n",
    "        batched_sparse_vectors, # Sparse vector embeddings\n",
    "        batched_dense_vectors,  # Dense vector embeddings\n",
    "    ]\n",
    "\n",
    "    # Insert the batch into Milvus\n",
    "    col.insert(batched_entities)\n",
    "\n",
    "print(\"Number of entities inserted:\", col.num_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your search query\n",
    "query = input(\"Enter your search query: \")\n",
    "print(query)\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embeddings = ef([query])\n",
    "print(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    AnnSearchRequest,\n",
    "    WeightedRanker,\n",
    ")\n",
    "\n",
    "def dense_search(col, query_dense_embedding, limit=5):\n",
    "    search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
    "    res = col.search(\n",
    "        [query_dense_embedding],\n",
    "        anns_field=\"dense_vector\",\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"],\n",
    "        param=search_params,\n",
    "    )[0]\n",
    "    return [hit.get(\"text\") for hit in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_search(col, query_sparse_embedding, limit=5):\n",
    "    search_params = {\n",
    "        \"metric_type\": \"IP\", \"params\": {}, }\n",
    "    res = col.search(\n",
    "        [query_sparse_embedding],\n",
    "        anns_field=\"sparse_vector\",\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"],\n",
    "        param=search_params,\n",
    "    )[0]\n",
    "    return [hit.get(\"text\") for hit in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    col,\n",
    "    query_dense_embedding,\n",
    "    query_sparse_embedding,\n",
    "    sparse_weight=1.0,\n",
    "    dense_weight=1.0,\n",
    "    limit=5,\n",
    "):\n",
    "    dense_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
    "    dense_req = AnnSearchRequest(\n",
    "        [query_dense_embedding], \"dense_vector\", dense_search_params, limit=limit\n",
    "    )\n",
    "    sparse_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
    "    sparse_req = AnnSearchRequest(\n",
    "        [query_sparse_embedding], \"sparse_vector\", sparse_search_params, limit=limit\n",
    "    )\n",
    "    rerank = WeightedRanker(sparse_weight, dense_weight)\n",
    "    res = col.hybrid_search(\n",
    "        [sparse_req, dense_req], rerank=rerank, limit=limit, output_fields=[\"text\"]\n",
    "    )[0]\n",
    "    return [hit.get(\"text\") for hit in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = dense_search(col, query_embeddings[\"dense\"][0])\n",
    "sparse_results = sparse_search(col, query_embeddings[\"sparse\"][[0]])\n",
    "hybrid_results = hybrid_search(\n",
    "    col,\n",
    "    query_embeddings[\"dense\"][0],\n",
    "    query_embeddings[\"sparse\"][[0]],\n",
    "    sparse_weight=0.7,\n",
    "    dense_weight=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dense_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sparse_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_text_formatting(ef, query, docs):\n",
    "    tokenizer = ef.model.tokenizer\n",
    "    query_tokens_ids = tokenizer.encode(query, return_offsets_mapping=True)\n",
    "    query_tokens = tokenizer.convert_ids_to_tokens(query_tokens_ids)\n",
    "    formatted_texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        ldx = 0\n",
    "        landmarks = []\n",
    "        encoding = tokenizer.encode_plus(doc, return_offsets_mapping=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])[1:-1]\n",
    "        offsets = encoding[\"offset_mapping\"][1:-1]\n",
    "        for token, (start, end) in zip(tokens, offsets):\n",
    "            if token in query_tokens:\n",
    "                if len(landmarks) != 0 and start == landmarks[-1]:\n",
    "                    landmarks[-1] = end\n",
    "                else:\n",
    "                    landmarks.append(start)\n",
    "                    landmarks.append(end)\n",
    "        close = False\n",
    "        formatted_text = \"\"\n",
    "        for i, c in enumerate(doc):\n",
    "            if ldx == len(landmarks):\n",
    "                pass\n",
    "            elif i == landmarks[ldx]:\n",
    "                if close:\n",
    "                    formatted_text += \"</span>\"\n",
    "                else:\n",
    "                    formatted_text += \"<span style='color:red'>\"\n",
    "                close = not close\n",
    "                ldx = ldx + 1\n",
    "            formatted_text += c\n",
    "        if close is True:\n",
    "            formatted_text += \"</span>\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return formatted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Dense search results\n",
    "display(Markdown(\"**Dense Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(ef, query, dense_results)\n",
    "for result in dense_results:\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse search results\n",
    "display(Markdown(\"\\n**Sparse Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(ef, query, sparse_results)\n",
    "for result in formatted_results:\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search results\n",
    "display(Markdown(\"\\n**Hybrid Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(ef, query, hybrid_results)\n",
    "for result in formatted_results:\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_template = f\"\"\"\n",
    "You are an expert assistant providing detailed and accurate information. The user has asked the following question:\n",
    "**Question:** {query}\n",
    "\n",
    "Below is relevant information retrieved from various sources:\n",
    "**Retrieved Information:**\n",
    "{hybrid_results}\n",
    "\n",
    "**Instruction:** Use only the query-relevant content from the retrieved information to answer the question. Focus on providing a comprehensive, informative response based solely on the given data. If any conflicting details are present, prioritize the most reliable and consistent information.\n",
    "\"\"\"\n",
    "final_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key='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' ,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_template,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
