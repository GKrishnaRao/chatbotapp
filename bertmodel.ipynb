{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import pinecone\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishna/Documents/PROJECTS/pythonproject/Github/chatbotapp/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the LegalBERT model\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"  # LegalBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'legalbertsearch' already exists\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=\"pcsk_4qGAdo_HJpbWqnXgMp73CihYSLJS6eRtfRcRDgA7jWsivdJP3aYAkjikFuqxhVabLVMhVj\",\n",
    "    environment=\"us-west-1\"\n",
    ")\n",
    "index_name = \"legalbertsearch\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Try to get the index\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Index '{index_name}' already exists\")\n",
    "except Exception as e:\n",
    "    # If index doesn't exist, create it\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-west-1'\n",
    "        )\n",
    "    )\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Index '{index_name}' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load and preprocess PDF\n",
    "def load_and_process_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split text into manageable chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "file_path = \"pdf/DCPR_2034_13-09-2024.pdf\"  # Replace with your legal PDF file\n",
    "chunks = load_and_process_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/y0npp81s6zb4z552xr0ztk600000gn/T/ipykernel_20006/2459162235.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
      "No sentence-transformers model found with name nlpaueb/legal-bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Generate embeddings using fine-tuned LegalBERT\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbeddings' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         index\u001b[38;5;241m.\u001b[39mupsert(vectors\u001b[38;5;241m=\u001b[39mto_upsert)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Use the function\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mupsert_to_pinecone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 10\u001b[0m, in \u001b[0;36mupsert_to_pinecone\u001b[0;34m(chunks, embedding_model)\u001b[0m\n\u001b[1;32m      8\u001b[0m texts \u001b[38;5;241m=\u001b[39m [chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m      9\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [chunk\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m---> 10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(texts)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m to_upsert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ids, embeddings, metadatas))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Upsert to Pinecone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PROJECTS/pythonproject/Github/chatbotapp/venv/lib/python3.12/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbeddings' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "# Function to create embeddings and upsert to Pinecone\n",
    "def upsert_to_pinecone(chunks, embedding_model):\n",
    "    batch_size = 100  # Adjust based on your needs\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        ids = [str(uuid.uuid4()) for _ in batch]\n",
    "        texts = [chunk.page_content for chunk in batch]\n",
    "        metadatas = [chunk.metadata for chunk in batch]\n",
    "        embeddings = embedding_model.encode(texts).tolist()\n",
    "        \n",
    "        to_upsert = list(zip(ids, embeddings, metadatas))\n",
    "        \n",
    "        # Upsert to Pinecone\n",
    "        index.upsert(vectors=to_upsert)\n",
    "\n",
    "# Use the function\n",
    "upsert_to_pinecone(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Optimize Pinecone Queries\n",
    "retriever = pinecone_index.as_retriever(search_kwargs={\"k\": 5})  # Retrieve top 5 results\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'realincgemma' already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nlpaueb/legal-bert-base-uncased. Creating a new one with mean pooling.\n",
      "/Users/krishna/Documents/PROJECTS/pythonproject/Github/chatbotapp/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying Pinecone: 'BertForPreTraining' object has no attribute 'encode'\n",
      "[{'score': 0, 'metadata': {}, 'text': \"Error: 'BertForPreTraining' object has no attribute 'encode'\"}]\n",
      "\n",
      "Result 1:\n",
      "Similarity Score: 0.0000\n",
      "Text: Error: 'BertForPreTraining' object has no attribute 'encode'...\n",
      "Query: What is the Minimum gap from the adjacent wall to the hand rail\n",
      "\n",
      "Relevant information:\n",
      "- Error: 'BertForPreTraining' object has no attribute 'encode'... (Similarity: 0.82)\n",
      "\n",
      "Based on the above information, please provide a comprehensive and accurate answer to the query.\n",
      "\n",
      "Final Answer:\n",
      "I'm sorry, but the provided information doesn't contain any details related to the minimum gap from the adjacent wall to the hand rail. Please provide relevant details or context to get an accurate response.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import numpy as np\n",
    "from transformers import AutoModelForPreTraining, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=\"pcsk_4qGAdo_HJpbWqnXgMp73CihYSLJS6eRtfRcRDgA7jWsivdJP3aYAkjikFuqxhVabLVMhVj\",\n",
    "    environment=\"us-west-1\"\n",
    ")\n",
    "index_name = \"realincgemma\"\n",
    "try:\n",
    "    # Try to get the index\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Index '{index_name}' already exists\")\n",
    "except Exception as e:\n",
    "    # If index doesn't exist, create it\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-west-1'\n",
    "        )\n",
    "    )\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Index '{index_name}' created successfully\")\n",
    "    \n",
    "# Load the same model you used to create embeddings\n",
    "model = SentenceTransformer('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load ChatGPT model from Hugging Face\n",
    "chatgpt_model_name = \"nlpaueb/legal-bert-base-uncased\"  # Replace with the actual Hugging Face model name for GPT-4\n",
    "tokenizer = AutoTokenizer.from_pretrained(chatgpt_model_name)\n",
    "chatgpt_model = AutoModelForCausalLM.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "def query_pinecone(query_text, top_k=5):\n",
    "    try:\n",
    "        \n",
    "    \n",
    "        # Generate embedding for the query\n",
    "        query_embedding = chatgpt_model.encode(query_text)\n",
    "        \n",
    "        # Convert to list, handling different possible types\n",
    "        if isinstance(query_embedding, np.ndarray):\n",
    "            query_embedding = query_embedding.tolist()\n",
    "        elif isinstance(query_embedding, torch.Tensor):\n",
    "            query_embedding = query_embedding.tolist()\n",
    "        elif not isinstance(query_embedding, list):\n",
    "            raise ValueError(f\"Unexpected embedding type: {type(query_embedding)}\")\n",
    "        \n",
    "        # Check index statistics\n",
    "        stats = index.describe_index_stats()\n",
    "        print(f\"Total vectors in index: {stats['total_vector_count']}\")\n",
    "\n",
    "        # Check if index is ready\n",
    "        if stats['total_vector_count'] == 0:\n",
    "            return [{'score': 0, 'metadata': {}, 'text': 'No data available in the index yet'}]\n",
    "        \n",
    "        #print(query_embedding)\n",
    "        # Query Pinecone\n",
    "        query_response = index.query(\n",
    "            vector=query_embedding,\n",
    "            namespace=\"\",\n",
    "            top_k=top_k,\n",
    "            include_values=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        #print(\"======== query_response start ========\")\n",
    "        #print(query_response)\n",
    "        #print(\"======== query_response end ========\")\n",
    "        \n",
    "        # Extract and return results\n",
    "        results = []\n",
    "        for match in query_response.matches:\n",
    "            results.append({\n",
    "                'score': match.score,\n",
    "                'metadata': match.metadata,\n",
    "                'text': match.metadata.get('text', 'No text available')\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Pinecone: {str(e)}\")\n",
    "        return [{'score': 0, 'metadata': {}, 'text': f'Error: {str(e)}'}]\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the Minimum gap from the adjacent wall to the hand rail\"\n",
    "results = query_pinecone(query)\n",
    "print(results)\n",
    "# Print results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Similarity Score: {result['score']:.4f}\")\n",
    "    print(f\"Text: {result['text'][:200]}...\")  # Print first 200 characters\n",
    "\n",
    "\n",
    "def refine_with_gpt4(context, results):\n",
    "    # Encode the context and results\n",
    "    context_embedding = model.encode(context)\n",
    "    result_embeddings = [model.encode(result['text']) for result in results]\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity([context_embedding], result_embeddings)[0]\n",
    "\n",
    "    # Sort results by similarity\n",
    "    sorted_results = sorted(zip(results, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a prompt with the most relevant information\n",
    "    prompt = f\"Query: {context}\\n\\nRelevant information:\\n\"\n",
    "    for result, similarity in sorted_results[:5]:  # Take top 5 most similar results\n",
    "        prompt += f\"- {result['text']}... (Similarity: {similarity:.2f})\\n\"\n",
    "    \n",
    "    prompt += \"\\nBased on the above information, please provide a comprehensive and accurate answer to the query.\"\n",
    "\n",
    "    print(prompt)\n",
    "    # Generate response using GPT-4\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # You can also use \"gpt-3.5-turbo\" if GPT-4 access is not available\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a Expert legal assistant that provides accurate information based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=300,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Usage\n",
    "if results:\n",
    "    refined_answer = refine_with_gpt4(query, results)\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(refined_answer)\n",
    "else:\n",
    "    print(\"No results to refine.\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
